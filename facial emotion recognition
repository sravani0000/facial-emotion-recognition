# Let us import the Libraries required.
import os
import cv2
import urllib
import numpy as np
from werkzeug.utils import secure_filename
from urllib.request import Request, urlopen
from flask import Flask, render_template, Response, request, redirect, flash, url_for
# Importing the required Classes/Functions from Modules defined.
from camera import VideoCamera
from Graphical_Visualisation import Emotion_Analysis
# Let us Instantiate the app
app = Flask(_name_)
###################################################################################
# We define some global parameters so that its easier for us to tweak when required.
# When serving files, we set the cache control max age to zero number of seconds
# for refreshing the Cache
app.config['SEND_FILE_MAX_AGE_DEFAULT'] = 0
UPLOAD_FOLDER = 'static'
ALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif'}
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
###################################################################################
# Some Utility Functions
# Flask provides native support for streaming responses through the use of generator
# functions. A generator is a special function that can be interrupted and resumed.
def gen(camera):
# for refreshing the Cache
"" "Helps in Passing frames from Web Camera to server"""
while True:
frame = camera.get_frame()
 yield (b'--frame\r\n'b'Content-Type: image/jpeg\r\n\r\n' + frame + b'\r\n\r\n')
def allowed_file(filename):
 """ Checks the file format when file is uploaded"""
 return ('.' in filename and
 filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS)
###################################################################################
def mood(result):
if result=="Happy":
return 'Since you are happy, lets keep up the good mood with some amazing music!'
elif result=="Sad":
return 'It seems that you are having a bad day, lets cheer you up with some amazing music!'
elif result=="Disgust":
return 'It seems something has got you feeling disgusted. Lets improve your mood with some great music!'
elif result=="Neutral":
return 'It seems like a normal day. Lets turn it into a great one with some amazing music!'
elif result=="Fear":
return 'You seem very scared. We are sure that some music will help!'
elif result=="Angry":
return 'You seem angry. Listening to some music will surely help you calm down!'
elif result=="Surprise":
return 'You seem surprised! Hopefully its some good news. Lets celebrate it with some great music!'
def provide_url(result):
if result=="Happy":
return 'https://open.spotify.com/playlist/1BVPSd4dynzdlIWehjvkPj'
elif result=="Sad":
return 'https://www.writediary.com/ '
elif result=="Disgust":
return 'https://open.spotify.com'
elif result=="Neutral":
return 'https://www.netflix.com/'
elif result=="Fear":
return 'https://www.youtube.com/watch?v=KWt2-lUpg-E'
elif result=="Angry":
return 'https://www.onlinemeditation.org/'
elif result=="Surprise":
return 'https://www.google.com/search?q=hotels+near+me&oq=hotels+&aqs=chrome.1.69i57j0i433i457j0i402l2j0i433l4j0l2.3606j0j7&sourceid=chrome&ie=UTF-8'
def activities(result):
if result == "Happy":
return '• Try out some dance moves'
elif result == "Sad":
return '• Write in a journal'
elif result == "Disgust":
return '• Listen soothing music'
elif result == "Neutral":
return '• Watch your favourite movie'
elif result == "Fear":
return '• Get a good sleep'
elif result == "Angry":
return '• Do meditation'
elif result == "Surprise":
return '• Give yourself a treat' \
@app.route('/')
def Start():
""" Renders the Home Page """
return render_template('Start.html')
@app.route('/video_feed')
def video_feed():
""" A route that returns a streamed response needs to return a Response object
that is initialized with the generator function."""
return Response(gen(VideoCamera()),
mimetype='multipart/x-mixed-replace; boundary=frame')
@app.route('/RealTime', methods=['POST'])
def RealTime():
""" Video streaming (Real Time Image from WebCam Video) home page."""
return render_template('RealTime.html')
@app.route('/takeimage', methods=['POST'])
def takeimage():
""" Captures Images from WebCam, saves them, does Emotion Analysis & renders. """
v = VideoCamera()
 _, frame = v.video.read()
save_to = "static/"
cv2.imwrite(save_to + "capture" + ".jpg", frame)
result = Emotion_Analysis("capture.jpg")
#When Classifier could not detect any Face.
if len(result) == 1:
return render_template('NoDetection.html', orig=result[0])
sentence = mood(result[3])
activity = activities(result[3])
link = provide_url(result[3])
return render_template('Visual.html', orig=result[0], pred=result[1], bar=result[2], music=result[3],
sentence=sentence, activity=activity, image=result[3], link=link)
@app.route('/ManualUpload', methods=['POST'])
def ManualUpload():
""" Manual Uploading of Images via URL or Upload """
return render_template('ManualUpload.html')
@app.route('/uploadimage', methods=['POST'])
def uploadimage():
""" Loads Image from System, does Emotion Analysis & renders."""
if request.method == 'POST':
# Check if the post request has the file part
if 'file' not in request.files:
flash('No file part')
return redirect(request.url)
file = request.files['file']
# If user does not select file, browser also
# submit an empty part without filename
if file.filename == '':
flash('No selected file')
return redirect(request.url)
# If user uploads the correct Image File
if file and allowed_file(file.filename):
# Pass it a filename and it will return a secure version of it.
# The filename returned is an ASCII only string for maximum portability.
filename = secure_filename(file.filename)
file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))
result = Emotion_Analysis(filename)
# When Classifier could not detect any Face.
if len(result) == 1:
return render_template('NoDetection.html', orig=result[0])
sentence = mood(result[3])
activity = activities(result[3])
link = provide_url(result[3])
return render_template('Visual.html', orig=result[0], pred=result[1], bar=result[2], music=result[3],
sentence=sentence, activity=activity, image=result[3], link=link)
@app.route('/imageurl', methods=['POST'])
def imageurl():
""" Fetches Image from URL Provided, does Emotion Analysis & renders."""
# Fetch the Image from the Provided URL
url = request.form['url']
req = Request(url,
headers={'User-Agent': 'Mozilla/5.0'})
# Reading, Encoding and Saving it to the static Folder
webpage = urlopen(req).read()
arr = np.asarray(bytearray(webpage), dtype=np.uint8)
img = cv2.imdecode(arr, -1)
save_to = "static/"
cv2.imwrite(save_to + "url.jpg", img)
result = Emotion_Analysis("url.jpg")
# When Classifier could not detect any Face.
if len(result) == 1:
return render_template('NoDetection.html', orig=result[0])
sentence = mood(result[3])
activity = activities(result[3])
link = provide_url(result[3])
return render_template('Visual.html', orig=result[0], pred=result[1], bar=result[2], music=result[3],
sentence=sentence, activity=activity, image=result[3], link=link)
if _name_ == '_main_':
app.run(debug=True)
CNN Code
#Initialising the CNN
model = Sequential()
# 1st Convolution Layer
# There are 64 (3,3) filters with "same" Padding and Shape of the Input_Image is (48,48,1)
model.add(Conv2D(64,(3,3), padding='same', input_shape=(48, 48,1)))
# Normalizing to speed up learning.
model.add(BatchNormalization())
# Applying Non Linear Activation Function "relu"
model.add(Activation('relu'))  
# Adding a Max Pool Layer of size (2,2)
model.add(MaxPooling2D(pool_size=(2, 2)))
# Dropout layer with 0.25 fraction of the input units to drop
model.add(Dropout(0.25))
# 2nd Convolution layer
# There are 128 (5,5) filters with "same" Padding 
model.add(Conv2D(128,(5,5), padding='same'))
# Normalizing to speed up learning.
model.add(BatchNormalization())
# Applying Non Linear Activation Function "relu"
model.add(Activation('relu'))
# Adding a Max Pool Layer of size (2,2)
model.add(MaxPooling2D(pool_size=(2, 2)))
# Dropout layer with 0.25 fraction of the input units to drop
model.add(Dropout(0.25))
# 3rd Convolution layer
# There are 512 (3,3) filters with "same" Padding 
model.add(Conv2D(512,(3,3), padding='same'))
# Normalizing to speed up learning.
model.add(BatchNormalization())
# Applying Non Linear Activation Function "relu"
model.add(Activation('relu'))
# Adding a Max Pool Layer of size (2,2)
model.add(MaxPooling2D(pool_size=(2, 2)))
# Dropout layer with 0.25 fraction of the input units to drop
model.add(Dropout(0.25))
# 4th Convolution layer
# There are 512 (3,3) filters with "same" Padding 
model.add(Conv2D(512,(3,3), padding='same'))
# Normalizing to speed up learning.
model.add(BatchNormalization())
# Applying Non Linear Activation Function "relu"
model.add(Activation('relu'))
# Adding a Max Pool Layer of size (2,2)
model.add(MaxPooling2D(pool_size=(2, 2)))
# Dropout layer with 0.25 fraction of the input units to drop 
model.add(Dropout(0.25))
# Flattening
model.add(Flatten())
# Fully connected layer with 256 nuerons
model.add(Dense(256))
# Normalizing to speed up learning.
model.add(BatchNormalization())
# Applying Non Linear Activation Function "relu"
model.add(Activation('relu'))
# Dropout layer with 0.25 fraction of the input units to drop
model.add(Dropout(0.25))
# Fully connected layer with 512 nuerons
model.add(Dense(512))
# Normalizing to speed up learning.
model.add(BatchNormalization())
# Applying Non Linear Activation Function "relu"
model.add(Activation('relu'))
# Dropout layer with 0.25 fraction of the input units to drop
model.add(Dropout(0.25))
# Adding a final Dense Layer with 7 outputs corresponding to 7 different emotions with a "softmax" Activation Function 
model.add(Dense(7, activation='softmax'))
CAMERA 
# Let us import the Libraries required.
import cv2
import numpy as n
from model import FacialExpressionMode
# Creating an instance of the class with the parameters as model and its weights.
model = FacialExpressionModel("model.json", "model_weights.h5")
# Loading the classifier from the file.
facec = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
class VideoCamera(object):
""" Takes the Real time Video, Predicts the Emotion using pre-trained model. """
def __init__(self):
self.video = cv2.VideoCapture(0)
def __del__(self):
self.video.release()
def get_frame(self):
"""It returns camera frames along with bounding boxes and predictions"""

 # Reading the Video and grasping the Frames
_, frame = self.video.read()
# Converting the Color image to Gray Scale
gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
       # Image size is reduced by 30% at each image scale.
       scaleFactor = 1.3
       # 5 neighbors should be present for each rectangle to be retained.
       minNeighbors = 5
# Detect the Faces in the given Image and store it in faces.
       faces = facec.detectMultiScale(gray_frame, scaleFactor, minNeighbors)
       # Iterating through all the faces detected
       for (x, y, w, h) in faces:
       # Taking the Face part in the Image as Region of Interest.
          roi = gray_frame[y:y+h, x:x+w]
          # Let us resize the Image accordingly to use pretrained model.
          roi = cv2.resize(roi, (48, 48))
          # Let us make the Prediction of Emotion present in the Image.
          prediction = model.predict_emotion(
          roi[np.newaxis, :, :, np.newaxis])
          # Custom Symbols to print with text of emotion.
          Symbols = {"Happy": ":)", "Sad": ":}", "Surprise": "!!",
          "Angry": "?", "Disgust": "#", "Neutral": ".", "Fear": "~"}
          # Defining the Parameters for putting Text on Image
Text = str(prediction) + Symbols[str(prediction)]
Text_Color = (180, 105, 255)
         Thickness = 2
Font_Scale = 1
Font_Type = cv2.FONT_HERSHEY_SIMPLEX
         # Inserting the Text on Image
cv2.putText(frame, Text, (x, y), Font_Type,
Font_Scale, Text_Color, Thickness
# Finding the Coordinates and Radius of Circle
xc = int((x + x+w)/2)
yc = int((y + y+h)/2)
radius = int(w/2)
# Drawing the Circle on the Image
         cv2.circle(frame, (xc, yc), radius, (0, 255, 0), Thickness)
         # Encoding the Image into a memory buffer
_, jpeg = cv2.imencode('.jpg', frame)
         # Returning the image as a bytes object
         return jpeg.tobytes()
Harr Cascade
<opencv_storage>
<cascade type_id="opencv-cascade-classifier"><stageType>BOOST</stageType> <featureType>HAAR</featureType>
<height>24</height>
<width>24</width>
<stageParams>
<maxWeakCount>211</maxWeakCount></stageParams>
<featureParams>
<maxCatCount>0</maxCatCount></featureParams>
<stageNum>25</stageNum>
<stages>
<_>
<maxWeakCount>9</maxWeakCount>
<stageThreshold>-5.0425500869750977e+00</stageThreshold>
<weakClassifiers>
<_>
<internalNodes>
0 -1 0 -3.1511999666690826e-02</internalNodes>
 <leafValues>
2.0875380039215088e+00 -2.2172100543975830e+00</leafValues></_>
<_>
<internalNodes>
0 -1 1 1.2396000325679779e-02</internalNodes>
<leafValues>
-1.8633940219879150e+00 1.3272049427032471e+00</leafValues></_>
<_>
<internalNodes>
0 -1 2 2.1927999332547188e-02</internalNodes>
<leafValues>
-1.5105249881744385e+00 1.0625729560852051e+00</leafValues></_>
<_>
<internalNodes>
0 -1 3 5.7529998011887074e-03</internalNodes>
<leafValues>
-8.7463897466659546e-01 1.1760339736938477e+00</leafValues></_>
<_>
<internalNodes>
0 -1 4 1.5014000236988068e-02</internalNodes>
<leafValues>
-7.7945697307586670e-01 1.2608419656753540e+00</leafValues></_>
<_>
<internalNodes>
0 -1 5 9.9371001124382019e-02</internalNodes>
<leafValues>
5.5751299858093262e-01 -1.8743000030517578e+00</leafValues></_>
 <_>
<internalNodes>
 0 -1 6 2.7340000960975885e-03</internalNodes>
<leafValues>
-1.6911929845809937e+00 4.4009700417518616e-01</leafValues></_>
<_>
<internalNodes>
0 -1 7 -1.8859000876545906e-02</internalNodes>
<leafValues>
-1.4769539833068848e+00 4.4350099563598633e-01</leafValues></_>
<_>
<internalNodes>
0 -1 8 5.9739998541772366e-03</internalNodes>
<leafValues>
-8.5909199714660645e-01 8.5255599021911621e-01</leafValues></_></weakClassifiers></_>
